Tensor：表示数据
Graph：表示计算任务
Operation：图中的节点，获得、计算、产生输出tensor
Session：会话，图需要在session里面启动，并执行operation


A quick complete tutorial to save and restore tensorflow models
https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/

tensorflow模型保存，调用
https://www.cnblogs.com/adong7639/p/7764769.html

model 打印所有权重
https://blog.csdn.net/AManFromEarth/article/details/81057577

从model导出权重，打印制定权重
https://blog.csdn.net/andeyeluguo/article/details/81190947

TF DEBUG
https://blog.csdn.net/u012436149/article/details/77479425
https://zhuanlan.zhihu.com/p/33264569

C++写的  DNN
https://github.com/Andy752/BackPropagationNeuralNetwork/blob/master/BackPropagationNeuralNetwork/test.cpp


TENSORFLOW API:
https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/layers/fully_connected


CNN
Deep Learning Specialization Course4 Week1 Assignment2
•	tf.nn.conv2d(X,W, strides = [1,s,s,1], padding = 'SAME'): given an input XX and a group of filters WW, this function convolves WW's filters on X. The third parameter ([1,s,s,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). Normally, you'll choose a stride of 1 for the number of examples (the first value) and for the channels (the fourth value), which is why we wrote the value as [1,s,s,1]. You can read the full documentation on conv2d.
•	tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = 'SAME'): given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window. For max pooling, we usually operate on a single example at a time and a single channel at a time. So the first and fourth value in [1,f,f,1] are both 1. You can read the full documentation on max_pool.
•	tf.nn.relu(Z): computes the elementwise ReLU of Z (which can be any shape). You can read the full documentation on relu.
•	tf.contrib.layers.flatten(P): given a tensor "P", this function takes each training (or test) example in the batch and flattens it into a 1D vector.
	If a tensor P has the shape (m,h,w,c), where m is the number of examples (the batch size), it returns a flattened tensor with shape (batch_size, k), where k=h×w×ck=h×w×c. "k" equals the product of all the dimension sizes other than the first dimension.
	For example, given a tensor with dimensions [100,2,3,4], it flattens the tensor to be of shape [100, 24], where 24 = 2 3 4. You can read the full documentation on flatten.
•	tf.contrib.layers.fully_connected(F, num_outputs): given the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation on full_connected.
In the last function above (tf.contrib.layers.fully_connected), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters.


